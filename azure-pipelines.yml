# Starter pipeline
# Start with a minimal pipeline that you can customize to build and deploy your code.
# Add steps that build, run tests, deploy, and more:
# https://aka.ms/yaml

trigger:
- release/*

variables:
  databricks.host: https://westeurope.azuredatabricks.net
  databricks.notebook.path: /Shared/ts_forecast
  databricks.cluster.name: small_73ML
  databricks.cluster.id: 
  databricks.cluster.spark_version: 7.3.x-cpu-ml-scala2.12
  databricks.cluster.node_type_id: Standard_DS3_v2
  databricks.cluster.driver_node_type_id: Standard_DS3_v2
  databricks.cluster.autotermination_minutes: 20
  databricks.cluster.workers.min: 1
  databricks.cluster.workers.max: 2
  databricks.job.train.name: Train
  databricks.job.train.id:

stages:
- stage: Build
  displayName: 'Train, Evaluate & Register Model'
  jobs:
  - job: Train
    displayName: 'Train, Evaluate & Register Model'
    pool:
      vmImage: 'ubuntu-latest'
    steps:
    - task: UsePythonVersion@0
      displayName: 'Use Python 3.6'
      inputs:
        versionSpec: '3.6'
        addToPath: true
        architecture: 'x64'
    - task: Bash@3
      displayName: 'Install Databricks CLI'
      inputs:
        targetType: 'inline'
        script: 'pip install -U databricks-cli'
    - task: Bash@3
      displayName: 'Configure Databricks CLI'
      inputs:
        targetType: 'inline'
        script: |
          # # We need to write the pipe the conf into databricks configure --token since
          # # that command only takes inputs from stdin. 
          # conf=`cat << EOM
          # $(databricks.host)
          # $(databricks.token)
          # EOM`
          
          # # For password auth there are three lines expected
          # # hostname, username, password
          # echo "$conf" | databricks configure --token
          databricks configure --token <<EOF
          $(databricks.host)
          $(databricks.token)
          EOF

    - task: Bash@3
      displayName: 'Detect list of clusters'
      inputs:
        targetType: 'inline'
        script: |
          databricks clusters list

    # - task: Bash@3
    #   displayName: 'Create Notebook Path'
    #   inputs:
    #     targetType: 'inline'
    #     script: 'databricks workspace mkdirs "$(databricks.notebook.path)"'
    # - task: Bash@3
    #   displayName: 'Import Notebooks to Shared space'
    #   inputs:
    #     targetType: 'inline'
    #     script: 'databricks workspace import_dir --overwrite notebooks "$(databricks.notebook.path)"'
    # - task: Bash@3
    #   displayName: 'Create / Get Cluster'
    #   inputs:
    #     targetType: 'inline'
    #     script: |
    #       cluster_id=$(databricks clusters list | grep "$(databricks.cluster.name)" | awk '{print $1}')
          
    #       if [ -z "$cluster_id" ]
    #       then
    #       JSON=`cat << EOM
    #       {
    #         "cluster_name": "$(databricks.cluster.name)",
    #         "spark_version": "$(databricks.cluster.spark_version)",
    #         "spark_conf": {
    #           "spark.databricks.delta.preview.enabled": "true"
    #         },
    #         "node_type_id": "$(databricks.cluster.node_type_id)",
    #         "driver_node_type_id": "$(databricks.cluster.driver_node_type_id)",
    #         "spark_env_vars": {
    #           "PYSPARK_PYTHON": "/databricks/python3/bin/python3"
    #         },
    #         "autotermination_minutes": $(databricks.cluster.autotermination_minutes),
    #         "enable_elastic_disk": true,
    #         "autoscale": {
    #           "min_workers": $(databricks.cluster.workers.min),
    #           "max_workers": $(databricks.cluster.workers.max)
    #         },
    #         "init_scripts_safe_mode": false
    #       }
    #       EOM`
          
    #       cluster_id=$(databricks clusters create --json "$JSON" | jq -r ".cluster_id")
    #       sleep 10
    #       fi
          
    #       echo "##vso[task.setvariable variable=databricks.cluster.id;]$cluster_id"
    # - task: Bash@3
    #   displayName: 'Start Cluster'
    #   inputs:
    #     targetType: 'inline'
    #     script: |
    #       echo "Checking Cluster State (Cluster ID: $(databricks.cluster.id))..."
    #       cluster_state=$(databricks clusters get --cluster-id "$(databricks.cluster.id)" | jq -r ".state")
    #       echo "Cluster State: $cluster_state"
          
    #       if [ $cluster_state == "TERMINATED" ]
    #       then
    #         echo "Starting Databricks Cluster..."
    #         databricks clusters start --cluster-id "$(databricks.cluster.id)"
    #         sleep 30
    #         cluster_state=$(databricks clusters get --cluster-id "$(databricks.cluster.id)" | jq -r ".state")
    #         echo "Cluster State: $cluster_state"
    #       fi
          
    #       while [ $cluster_state == "PENDING" ]
    #       do
    #         sleep 30
    #         cluster_state=$(databricks clusters get --cluster-id "$(databricks.cluster.id)" | jq -r ".state")
    #         echo "Cluster State: $cluster_state"
    #       done
          
    #       if [ $cluster_state == "RUNNING" ]
    #       then
    #         exit 0
    #       else
    #         exit 1
    #       fi        
    # - task: Bash@3
    #   displayName: 'Create / Get Training Job'
    #   inputs:
    #     targetType: 'inline'
    #     script: |
    #       job_id=$(databricks jobs list | grep "$(databricks.job.train.name)" | awk '{print $1}')
          
    #       if [ -z "$job_id" ]
    #       then
    #       echo "Creating $(databricks.job.train.name) job..."
    #       JSON=`cat << EOM
    #       {
    #         "notebook_task": {
    #           "notebook_path": "$(databricks.notebook.path)/training_new_petastorm",
    #           "base_parameters": {
    #             "alpha": "0.5",
    #             "l1_ratio": "0.5"
    #           }
    #         },
    #         "existing_cluster_id": "$(databricks.cluster.id)",
    #         "name": "$(databricks.job.train.name)",
    #         "max_concurrent_runs": 1,
    #         "timeout_seconds": 86400,
    #         "libraries": [],
    #         "email_notifications": {}
    #       }
    #       EOM`
          
    #       job_id=$(databricks jobs create --json "$JSON" | jq ".job_id")
    #       fi
          
    #       echo "##vso[task.setvariable variable=databricks.job.train.id;]$job_id"
    # - task: Bash@3
    #   displayName: 'Run Training Jobs'
    #   inputs:
    #     targetType: 'inline'
    #     script: |
    #       echo "Running job with ID $(databricks.job.train.id) with alpha=0.5, l1_ratio=0.5..."
    #       run_id1=$(databricks jobs run-now --job-id $(databricks.job.train.id) --notebook-params '{ "alpha": "0.5", "l1_ratio": "0.5" }' | jq ".run_id")
    #       echo "  Run ID: $run_id1"
    #       run_state=$(databricks runs get --run-id $run_id1 | jq -r ".state.life_cycle_state")
    #       echo "Run State (ID $run_id1): $run_state"
    #       while [ $run_state == "RUNNING" -o $run_state == "PENDING" ]
    #       do
    #         sleep 30
    #         run_state=$(databricks runs get --run-id $run_id1 | jq -r ".state.life_cycle_state")
    #         echo "Run State (ID $run_id1): $run_state"
    #       done
    #       result_state1=$(databricks runs get --run-id $run_id1 | jq -r ".state.result_state")
    #       state_message1=$(databricks runs get --run-id $run_id1 | jq -r ".state.state_message")
    #       echo "Result State (ID $run_id1): $result_state1, Message: $state_message1"
          
    #       if [ $result_state1 == "SUCCESS" ]
    #       then
    #         exit 0
    #       else
    #         exit 1
    #       fi